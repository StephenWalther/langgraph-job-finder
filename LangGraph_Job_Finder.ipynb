{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQBYNV+3yg6LpJ8ZuZ79SE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StephenWalther/langgraph-job-finder/blob/main/LangGraph_Job_Finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangGraph Job Finder\n",
        "This app illustrates how to use LangGraph with a Large Language Model (LLM) to retrieve a list of jobs, generate personalized cover letters for each job, and save the cover letters to Google Drive. This app also illustrates using LangGraph with Human in the Loop to get human input at a crucial step."
      ],
      "metadata": {
        "id": "mifXkcu_N01o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHK_ArOoIShv"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "\"\"\"\n",
        "  These are global notebook settings such as LLM instructions and Google\n",
        "  Drive paths.\n",
        "\"\"\"\n",
        "\n",
        "GOOGLE_DRIVE_PATH = '/content/drive'\n",
        "APP_PATH = GOOGLE_DRIVE_PATH + '/MyDrive/langgraph-job-finder'\n",
        "RESUME_PATH = APP_PATH + '/resume.pdf'\n",
        "OUTPUT_PATH = APP_PATH + '/output'\n",
        "PREVIOUS_PATH = APP_PATH + '/previous'\n",
        "JOB_SEARCH_URL = (\n",
        "    \"https://www.linkedin.com/jobs/search/?\"\n",
        "    \"keywords=Product%20Manager%20AI\"\n",
        "    \"&location=Austin%2C%20Texas%2C%20United%20States\"\n",
        "    \"&f_WT=2%2C3\" # 2 is Remote, 3 is Hybrid\n",
        "    \"&f_TPR=r604800\" # Posted in last week\n",
        "    \"&position=1\"\n",
        "    \"&pageNum=0\"\n",
        ")\n",
        "\n",
        "\n",
        "COVER_LETTER_INSTRUCTIONS = \"\"\"\n",
        "  You are a helpful assistant that writes cover letter used in an email. Do not\n",
        "  include placeholders. Keep the cover letter concise (no more than 3 paragraphs).\n",
        "  Do not mention the job ID in the cover letter. Lead with the reason that I am the\n",
        "  best match for the job given my resume and the job description. Do not make up\n",
        "  any qualifications that are not explicitly listed in the resume. Always start\n",
        "  with a salutation.\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get secrets\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "LINKEDIN_EMAIL = userdata.get('LINKEDIN_EMAIL')\n",
        "if LINKEDIN_EMAIL is None:\n",
        "  raise ValueError('LINKEDIN_EMAIL not set in secrets')\n",
        "\n",
        "LINKEDIN_PASSWORD = userdata.get('LINKEDIN_PASSWORD')\n",
        "if LINKEDIN_PASSWORD is None:\n",
        "  raise ValueError('LINKEDIN_PASSWORD not set in secrets')\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "if OPENAI_API_KEY is None:\n",
        "  raise ValueError('OPENAI_API_KEY not set in secrets')\n",
        "\n",
        "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "if ANTHROPIC_API_KEY is None:\n",
        "  raise ValueError('ANTHROPIC_API_KEY not set in secrets')\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "if GOOGLE_API_KEY is None:\n",
        "  raise ValueError('GOOGLE_API_KEY not set in secrets')"
      ],
      "metadata": {
        "id": "uywP5dpOguNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "\n",
        "%pip install -U langchain_openai\n",
        "%pip install -U langchain_anthropic\n",
        "%pip install -U langchain-google-genai\n",
        "%pip install -U openai\n",
        "%pip install -U pdfplumber\n",
        "%pip install -U reportlab\n",
        "%pip install PyPDF2\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb\n",
        "!apt-get install -y x11-xserver-utils\n",
        "!apt-get install -y xephyr\n",
        "\n",
        "\n",
        "%pip install pyvirtualdisplay\n",
        "\n",
        "%pip install -U langgraph langsmith\n",
        "%pip install -U openai\n",
        "%pip install -U playwright\n",
        "!playwright install"
      ],
      "metadata": {
        "id": "7SmdZF4vX8cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable virtual display for headful browser\n",
        "\"\"\"\n",
        "  LinkedIn will act differently when accessed with a headless browser.\n",
        "  Therefore, we are instantiating a 1920x1080 browser with a virtual display.\n",
        "\"\"\"\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from playwright.sync_api import sync_playwright\n",
        "import os\n",
        "\n",
        "#display = Display(visible=1, size=(1920, 1080), backend='xvfb', extra_args=['-ac'])\n",
        "display = Display(visible=1, size=(3840, 2160), backend='xvfb', extra_args=['-ac'])\n",
        "display.start()"
      ],
      "metadata": {
        "id": "PZ5GvF_dKlOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "\"\"\"\n",
        "  We are using Google Drive as a database (persistent data store)\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount(GOOGLE_DRIVE_PATH)"
      ],
      "metadata": {
        "id": "WC_38sR3JEo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LangGraph App folder\n",
        "\"\"\"\n",
        "  Create the standard Google Drive folders:\n",
        "    /langgraph-job-finder\n",
        "    /langgraph-job-finder/output\n",
        "    /langgraph-job-finder/previous\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if not os.path.exists(APP_PATH):\n",
        "  os.makedirs(APP_PATH)\n",
        "  print(f\"Folder '{APP_PATH}' created successfully.\")\n",
        "else:\n",
        "  print(f\"Folder '{APP_PATH}' already exists.\")\n",
        "\n",
        "if not os.path.exists(PREVIOUS_PATH):\n",
        "  os.makedirs(PREVIOUS_PATH)\n",
        "  print(f\"Folder '{PREVIOUS_PATH}' created successfully.\")\n",
        "else:\n",
        "  print(f\"Folder '{PREVIOUS_PATH}' already exists.\")"
      ],
      "metadata": {
        "id": "lPV0yMg0QJww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Resume\n",
        "\"\"\"\n",
        "  If a resume.pdf does not exist in the app folder, prompt the user\n",
        "  to upload one.\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "def load_or_upload_resume(resume_path):\n",
        "  \"\"\"Loads a resume from the given path or prompts the user to upload one.\n",
        "\n",
        "  Args:\n",
        "    resume_path: The path to the resume file.\n",
        "\n",
        "  Returns:\n",
        "    The path to the loaded or uploaded resume file.\n",
        "  \"\"\"\n",
        "  if os.path.exists(resume_path):\n",
        "    print(f\"Resume found at '{resume_path}'. Loading...\")\n",
        "    return resume_path\n",
        "  else:\n",
        "    print(f\"Resume not found at '{resume_path}'. Please upload your resume:\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "      with open(resume_path, 'wb') as f:\n",
        "        f.write(uploaded[filename])\n",
        "      print(f\"Resume saved to '{resume_path}'.\")\n",
        "      return resume_path\n",
        "\n",
        "resume_path = load_or_upload_resume(RESUME_PATH)\n",
        "print(f\"Using resume from: {resume_path}\")\n"
      ],
      "metadata": {
        "id": "Q9WJMFfyOJSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Google Drive folder for today\n",
        "\"\"\"\n",
        "  Create a new folder with today's date as the name to store all of the\n",
        "  generated cover letters.\n",
        "\"\"\"\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "def create_dated_folder(output_path):\n",
        "  \"\"\"Creates a new folder with today's date as the name.\n",
        "\n",
        "  Args:\n",
        "    output_path: The base path where the folder should be created.\n",
        "\n",
        "  Returns:\n",
        "    The path to the created folder.\n",
        "  \"\"\"\n",
        "  today = datetime.date.today().strftime('%Y-%m-%d')\n",
        "  folder_path = os.path.join(output_path, today)\n",
        "\n",
        "  if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "    print(f\"Folder '{folder_path}' created successfully.\")\n",
        "  else:\n",
        "    print(f\"Folder '{folder_path}' already exists.\")\n",
        "\n",
        "  return folder_path\n",
        "\n",
        "# create folder:\n",
        "today_folder_path = create_dated_folder(OUTPUT_PATH)\n",
        "print(f\"Today's output folder: {today_folder_path}\")"
      ],
      "metadata": {
        "id": "vmTw1uP1LkAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global functions\n",
        "\"\"\"\n",
        "  Functions that are used throughout the notebook.\n",
        "\"\"\"\n",
        "\n",
        "async def screen_shot(page, name, show=True):\n",
        "  # generate and display image\n",
        "  await page.screenshot(path=f\"{name}.png\")\n",
        "  if show:\n",
        "    display(Image(filename=f\"{name}.png\"))\n",
        "\n",
        "  # save complete HTML\n",
        "  html = await page.content()\n",
        "  with open(f\"{name}.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html)\n",
        "\n",
        "\n",
        "def get_cover_letter_path_txt(job_folder_path, provider):\n",
        "  return os.path.join(job_folder_path, f\"{provider}_cover_letter.txt\")\n",
        "\n",
        "def get_cover_letter_path_pdf(job_folder_path, provider):\n",
        "  return os.path.join(job_folder_path, f\"{provider}_cover_letter.pdf\")\n",
        "\n",
        "def get_resume_with_cover_letter_path_pdf(job_folder_path, provider):\n",
        "  return os.path.join(job_folder_path, f\"{provider}_resume_with_cover_letter.pdf\")"
      ],
      "metadata": {
        "id": "tiYApm9aIe_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LangGraph State\n",
        "\"\"\"\n",
        "  Did not end up using a state object because used Google Drive as persistent\n",
        "  state instead (which worked better for this scenario)\n",
        "\"\"\"\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing_extensions import Annotated, TypedDict, List, Optional\n",
        "\n",
        "class Job(TypedDict):\n",
        "    title: str\n",
        "    company: str\n",
        "    description: str\n",
        "    url: str\n",
        "    isGoodFit: Optional[bool]\n",
        "    coverLetter: Optional[str]\n",
        "    isApproved: Optional[bool]\n",
        "\n",
        "class State(TypedDict):\n",
        "    resume: str\n",
        "    jobs: List[Job]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)"
      ],
      "metadata": {
        "id": "JMtflVkgMtQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangGraph Node - login - Login to LinkedIn\n",
        "\"\"\"\n",
        "  You need an existing LinkedIn email and password stored in secrets. This node\n",
        "  will request a LinkedIn verification PIN from your email if LinkedIn\n",
        "  requires it.\n",
        "\"\"\"\n",
        "\n",
        "async def login(state: State):\n",
        "  print(\"login node started\")\n",
        "\n",
        "  # Navigate to LinkedIn login page\n",
        "  await page.goto('https://www.linkedin.com/login')\n",
        "  await screen_shot(page, \"linkedin_login\")\n",
        "  print(\"Navigated to LinkedIn login page\")\n",
        "\n",
        "  # Fill in credentials\n",
        "  await page.fill('#username', LINKEDIN_EMAIL)\n",
        "  await page.fill('#password', LINKEDIN_PASSWORD)\n",
        "\n",
        "  # Click the sign in button\n",
        "  await page.wait_for_timeout(random.uniform(500, 1000))\n",
        "  await page.click('button[type=\"submit\"]')\n",
        "  await screen_shot(page, \"linkedin_login_submit\")\n",
        "  print(\"Clicked authenticate button\")\n",
        "\n",
        "\n",
        "  # if verification required, ask user for code from email\n",
        "  # Check if the <input> element with name=\"pin\" exists\n",
        "  pin_locator = page.locator('input[name=\"pin\"]')\n",
        "  exists = await pin_locator.count() > 0\n",
        "  if exists:\n",
        "      print(\"The input element with name='pin' exists.\")\n",
        "\n",
        "      # ask user for pin\n",
        "      pin = input(\"Enter LinkedIn PIN code from email\")\n",
        "\n",
        "       # Fill in credentials\n",
        "      await page.fill('input[name=\"pin\"]', pin)\n",
        "\n",
        "      # Click the sign in button\n",
        "      await page.click('button[type=\"submit\"]')\n",
        "      await screen_shot(page, \"pin_submit\")\n",
        "      print(\"Clicked pin button\")\n",
        "\n",
        "  else:\n",
        "      print(\"The input element with name='pin' does not exist.\")\n",
        "\n",
        "  print(\"login node done\")\n",
        "  return"
      ],
      "metadata": {
        "id": "0QfqCN-vLHew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangGraph Node - fetch_jobs - Fetch job list using LinkedIn job search criteria\n",
        "\"\"\"\n",
        "  Navigate to job search results page and retrieve a list of jobs. Matching\n",
        "  jobs are stored in your Google Drive.\n",
        "\"\"\"\n",
        "import os\n",
        "from playwright.async_api import async_playwright\n",
        "import random\n",
        "from IPython.display import Image\n",
        "\n",
        "def extract_job_id(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract the job ID from a LinkedIn job URL.\n",
        "    Example URL: /jobs/view/4093827142/?eBP=...\n",
        "    Returns: '4093827142'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Split the URL by '/' and get the job ID segment\n",
        "        segments = url.strip('/').split('/')\n",
        "        # The job ID should be after 'view' in the URL\n",
        "        if 'view' in segments:\n",
        "            view_index = segments.index('view')\n",
        "            job_id = segments[view_index + 1].split('?')[0]  # Remove any query parameters\n",
        "            return job_id\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting job ID: {e}\")\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "async def fetch_jobs(state):\n",
        "    print(\"fetch_jobs node started\")\n",
        "\n",
        "    # Navigate to Jobs page with search parameters\n",
        "    # Using URL parameters for more precise control\n",
        "    await page.goto(url=JOB_SEARCH_URL, wait_until='domcontentloaded')\n",
        "    print(\"Navigated to job search results\")\n",
        "\n",
        "    # save as jobs\n",
        "    await screen_shot(page, \"jobs\")\n",
        "\n",
        "    # Get links\n",
        "    await page.wait_for_timeout(random.uniform(500, 1000))\n",
        "    job_links = await page.locator('a.job-card-container__link').all()\n",
        "    print(\"Getting each job link. found \" + str(len(job_links)) + \" links\")\n",
        "\n",
        "    # Click on each job link\n",
        "    for job_link in job_links:\n",
        "        job_id = extract_job_id(await job_link.get_attribute('href'))\n",
        "        print(f\"Job ID: {job_id}\")\n",
        "\n",
        "        # if the job id is already in the PREVIOUS folder, skip\n",
        "        if os.path.exists(f\"{PREVIOUS_PATH}/{job_id}.html\"):\n",
        "            print(f\"Skipping job ID: {job_id} because it already exists\")\n",
        "            continue\n",
        "\n",
        "        # Before clicking, check for and dismiss any modal overlays:\n",
        "        try:\n",
        "            # This selector was updated based on the call log in the error\n",
        "            dismiss_button = page.locator('button[aria-label=\"Dismiss\"] >> visible=true')\n",
        "            if await dismiss_button.is_visible():\n",
        "                await dismiss_button.click()\n",
        "                print(\"Dismissed modal overlay with aria-label 'Dismiss'\")\n",
        "                await page.wait_for_timeout(1000)\n",
        "        except Exception as e:\n",
        "            print(f\"Error handling modal overlay: {e}\")\n",
        "            # This was added to continue to the next job if error handling modals failed\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Now attempt the click on the job link\n",
        "        await page.wait_for_timeout(random.uniform(500, 1000))\n",
        "        await job_link.click(timeout=60000) # Increased timeout to 60 seconds\n",
        "        print(\"Clicked job link\")\n",
        "        await screen_shot(page, f\"job_link_clicked_{job_id}\")\n",
        "\n",
        "        # get job details\n",
        "        job_details = await page.locator('div.jobs-details').inner_html()\n",
        "        print(\"got job details\")\n",
        "\n",
        "        # Save job details to file\n",
        "        with open(f\"{today_folder_path}/{job_id}_job_details.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(job_details)\n",
        "\n",
        "        # Add to previous\n",
        "        with open(f\"{PREVIOUS_PATH}/{job_id}.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(job_details)\n",
        "\n"
      ],
      "metadata": {
        "id": "PHPePr5YKhWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangGraph Node - extract_job_info\n",
        "\"\"\"\n",
        "  Convert the HTML job description to JSON using OpenAI.\n",
        "\"\"\"\n",
        "# https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html#langchain_openai.chat_models.base.ChatOpenAI\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from openai import ContentFilterFinishReasonError\n",
        "\n",
        "job_parse_instructions = \"\"\"Parse the HTML job description and return the job details as JSON\n",
        "  using the following format:\n",
        "  {\n",
        "    job_title: string,\n",
        "    job_description: string,\n",
        "    job_location: string,\n",
        "    job_company: string,\n",
        "    job_application_url: string,\n",
        "    job_id: string,\n",
        "    job_hiring_managers: string\n",
        "    job_pay_range: string\n",
        "  }\n",
        "  \"\"\"\n",
        "\n",
        "async def extract_job_info(state):\n",
        "  print(\"extract_job_info started\")\n",
        "\n",
        "  # Init LLM\n",
        "  llm = ChatOpenAI(model=\"gpt-4o\", api_key=OPENAI_API_KEY)\n",
        "  json_llm = llm.bind(response_format={\"type\": \"json_object\"})\n",
        "\n",
        "  # retrieve each file from the data folder with a name that matches the pattern 'job_details_*.html'\n",
        "  for file in os.listdir(today_folder_path):\n",
        "      if file.endswith('_job_details.html'):\n",
        "          print(f\"Processing file: {file}\")\n",
        "          json_path = f'{today_folder_path}/{file.replace(\".html\", \".json\")}'\n",
        "\n",
        "          # if JSON already extracted, skip\n",
        "          if os.path.exists(json_path):\n",
        "              print(f\"Skipping job: {json_path} because it already exists\")\n",
        "              continue\n",
        "\n",
        "          with open(f'{today_folder_path}/{file}', 'r', encoding='utf-8') as f:\n",
        "              html_content = f.read()\n",
        "\n",
        "              messages=[\n",
        "                (\"system\", job_parse_instructions + html_content)\n",
        "              ]\n",
        "\n",
        "              # parse the html by passing the file to OpenAI and extract the job details into JSON\n",
        "              try:\n",
        "                response = await json_llm.ainvoke(messages)\n",
        "                print(response.content)\n",
        "\n",
        "                # save the response to a new file in a folder beneath the data folder with the current date with the same name but with a .json extension\n",
        "                with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                    f.write(response.content)\n",
        "              except ContentFilterFinishReasonError as e:\n",
        "                print(f\"Content filter error: {e}\")\n",
        "\n",
        "# uncomment to run this node independently\n",
        "# await extract_job_info(None)"
      ],
      "metadata": {
        "id": "nQhw0HPKK0Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangGraph Node - generate_cover_letters\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\"\"\"\n",
        "  Generate the following files for each job:\n",
        "    - cover letter (text version)\n",
        "    - cover letter (PDF version)\n",
        "    - resume with cover letter (PDF)\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import pdfplumber\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "\n",
        "# Convert PDF to text\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "  with pdfplumber.open(pdf_path) as pdf:\n",
        "    text = ''\n",
        "    for page in pdf.pages:\n",
        "        text += page.extract_text() or ''\n",
        "  return text\n",
        "\n",
        "def save_cover_letter_to_txt(cover_letter_text, cover_letter_path):\n",
        "  with open(cover_letter_path, 'w') as f:\n",
        "    f.write(cover_letter_text)\n",
        "\n",
        "def save_cover_letter_to_pdf(cover_letter_text, cover_letter_path):\n",
        "  styles = getSampleStyleSheet()\n",
        "  normal_style = styles[\"Normal\"]\n",
        "  story = []\n",
        "  for paragraph in cover_letter_text.split('\\n'):\n",
        "      story.append(Paragraph(paragraph, normal_style))\n",
        "      story.append(Spacer(1, 12))  # Add space between paragraphs\n",
        "\n",
        "  doc = SimpleDocTemplate(cover_letter_path, pagesize=letter)\n",
        "  doc.build(story)\n",
        "\n",
        "def save_resume_with_cover_letter(resume_with_cover_letter_path_pdf, cover_letter_path_pdf):\n",
        "  merger = PdfWriter()\n",
        "\n",
        "  # Add cover letter first\n",
        "  merger.append(PdfReader(cover_letter_path_pdf, 'rb'))\n",
        "\n",
        "  # Add resume\n",
        "  print(f\"resume path: {RESUME_PATH}\")\n",
        "  merger.append(PdfReader(open(RESUME_PATH, 'rb')))\n",
        "\n",
        "  # Write the merged PDF\n",
        "  merger.write(resume_with_cover_letter_path_pdf)\n",
        "  merger.close()\n",
        "\n",
        "async def generate_cover_letter_by_provider(provider, llm, resume_text, job_details, job_folder_path):\n",
        "  print(f\"generate_cover_letter_by_provider started for provider {provider}\")\n",
        "\n",
        "  # generate paths\n",
        "  cover_letter_path_txt = get_cover_letter_path_txt(job_folder_path, provider)\n",
        "  cover_letter_path_pdf = get_cover_letter_path_pdf(job_folder_path, provider)\n",
        "  resume_with_cover_letter_path_pdf = get_resume_with_cover_letter_path_pdf(job_folder_path, provider)\n",
        "\n",
        "  # if cover letters already generated, skip\n",
        "  if os.path.exists(cover_letter_path_txt):\n",
        "    print(f\"Skipping job because it already exists\")\n",
        "    return\n",
        "\n",
        "\n",
        "  # Build request payload\n",
        "  messages=[\n",
        "    (\"system\", COVER_LETTER_INSTRUCTIONS),\n",
        "    (\"human\", f\"Write a cover letter for the following job: {job_details} using the following resume: {resume_text}\")\n",
        "  ]\n",
        "\n",
        "  # send resume_text and job_details to OpenAI to write a cover letter\n",
        "  response = await llm.ainvoke(messages)\n",
        "  cover_letter_text = response.content.strip()\n",
        "  print(cover_letter_text)\n",
        "\n",
        "  # save the cover letter to a txt file '*_cover_letter.txt'\n",
        "  save_cover_letter_to_txt(cover_letter_text, cover_letter_path_txt)\n",
        "\n",
        "  # Save the cover letter to a pdf file '*_cover_letter.pdf'\n",
        "  save_cover_letter_to_pdf(cover_letter_text, cover_letter_path_pdf)\n",
        "\n",
        "  # Merge the cover letter PDF and the resume PDF\n",
        "  save_resume_with_cover_letter(resume_with_cover_letter_path_pdf, cover_letter_path_pdf)\n",
        "\n",
        "async def generate_cover_letters(state):\n",
        "  print(\"generate_cover_letters node started\")\n",
        "\n",
        "  # Init LLMs\n",
        "  llm_openai = ChatOpenAI(model=\"gpt-4o\", api_key=OPENAI_API_KEY)\n",
        "  llm_anthropic = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", anthropic_api_key=ANTHROPIC_API_KEY)\n",
        "  llm_google = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "  for file in os.listdir(today_folder_path):\n",
        "    if file.endswith('.json'):\n",
        "      with open(f'{today_folder_path}/{file}', 'r', encoding='utf-8') as f:\n",
        "        print(f'Processing {file}')\n",
        "\n",
        "        # Load job details\n",
        "        job_details = json.load(f)\n",
        "        job_id = job_details.get(\"job_id\", \"id not found\")\n",
        "\n",
        "        # Calculate job folder path\n",
        "        job_folder_path = os.path.join(today_folder_path, job_id)\n",
        "        if not os.path.exists(job_folder_path):\n",
        "          os.makedirs(job_folder_path)\n",
        "\n",
        "        # Get resume text instead of binary PDF data\n",
        "        resume_text = extract_text_from_pdf(RESUME_PATH)\n",
        "\n",
        "        await generate_cover_letter_by_provider('openai', llm_openai, resume_text, job_details, job_folder_path)\n",
        "        await generate_cover_letter_by_provider('anthropic', llm_anthropic, resume_text, job_details, job_folder_path)\n",
        "        await generate_cover_letter_by_provider('google', llm_google, resume_text, job_details, job_folder_path)\n",
        "\n",
        "\n",
        "# uncomment to run this node independently\n",
        "# await generate_cover_letters(None)"
      ],
      "metadata": {
        "id": "ljbY-YU8Mg1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangGraph Node - report\n",
        "\"\"\"\n",
        "  Generate report.html file with job application links for\n",
        "  each job.\n",
        "\"\"\"\n",
        "async def report(state):\n",
        "  report_file_path = os.path.join(today_folder_path, \"report.html\")\n",
        "  html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "        <title>Job Application Report</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Job Application Report</h1>\n",
        "        <ul>\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  for file in os.listdir(today_folder_path):\n",
        "    if file.endswith('.json'):\n",
        "      with open(f'{today_folder_path}/{file}', 'r', encoding='utf-8') as f:\n",
        "        print(f'Processing {file}')\n",
        "        job_details = json.load(f)\n",
        "\n",
        "        # Extract job details\n",
        "        job_title = job_details.get(\"job_title\", \"Title not found\")\n",
        "        job_application_url = job_details.get(\"job_application_url\", \"#\")\n",
        "        job_id = job_details.get(\"job_id\", \"id not found\")\n",
        "        job_company = job_details.get(\"job_company\", \"job_company not found\")\n",
        "        job_pay_range = job_details.get(\"job_pay_range\", \"job_pay_range not found\")\n",
        "\n",
        "        # If missing job_id folder then skip\n",
        "        job_folder_path = os.path.join(today_folder_path, job_id)\n",
        "        if not os.path.exists(job_folder_path):\n",
        "          continue\n",
        "\n",
        "        # Generate paths to cover letters and resumes\n",
        "        openai_cover_letter_path_pdf = f\"{job_id}/openai_cover_letter.pdf\"\n",
        "        openai_resume_with_cover_letter_path_pdf = f\"{job_id}/openai_resume_with_cover_letter.pdf\"\n",
        "\n",
        "        anthropic_cover_letter_path_pdf = f\"{job_id}/anthropic_cover_letter.pdf\"\n",
        "        anthropic_resume_with_cover_letter_path_pdf = f\"{job_id}/anthropic_resume_with_cover_letter.pdf\"\n",
        "\n",
        "        google_cover_letter_path_pdf = f\"{job_id}/google_cover_letter.pdf\"\n",
        "        google_resume_with_cover_letter_path_pdf = f\"{job_id}/google_resume_with_cover_letter.pdf\"\n",
        "\n",
        "        # Add to HTML content\n",
        "        html_content += f\"\"\"\n",
        "            <li>\n",
        "                <h2>{job_id} - {job_company} - {job_title}</h2>\n",
        "                <p><small>{job_pay_range}</small></p>\n",
        "                <p><a href=\"{job_application_url}\" target=\"_blank\">Apply Now</a></p>\n",
        "                <p>Cover Letter PDF:\n",
        "                  <a href=\"{openai_cover_letter_path_pdf}\" target=\"_blank\">OpenAI</a> /\n",
        "                  <a href=\"{anthropic_cover_letter_path_pdf}\" target=\"_blank\">Anthropic</a> /\n",
        "                  <a href=\"{google_cover_letter_path_pdf}\" target=\"_blank\">Google</a>\n",
        "                </p>\n",
        "            </li>\n",
        "        \"\"\"\n",
        "\n",
        "  html_content += \"\"\"\n",
        "        </ul>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "  # Write the HTML content to file\n",
        "  with open(report_file_path, 'w', encoding='utf-8') as report_file:\n",
        "      report_file.write(html_content)\n",
        "  print(f\"Report generated at: {report_file_path}\")\n",
        "\n",
        "# uncomment to run this node independently\n",
        "# await report(None)"
      ],
      "metadata": {
        "id": "ND8qVHtXv-uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LangGraph graph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()\n",
        "\n",
        "# add nodes\n",
        "graph_builder.add_node(\"login\", login)\n",
        "graph_builder.add_node(\"fetch_jobs\", fetch_jobs)\n",
        "graph_builder.add_node(\"extract_job_info\", extract_job_info)\n",
        "graph_builder.add_node(\"generate_cover_letters\", generate_cover_letters)\n",
        "graph_builder.add_node(\"report\", report)\n",
        "\n",
        "\n",
        "# add edges\n",
        "graph_builder.add_edge(START, \"login\")\n",
        "graph_builder.add_edge(\"login\", \"fetch_jobs\")\n",
        "graph_builder.add_edge(\"fetch_jobs\", \"extract_job_info\")\n",
        "graph_builder.add_edge(\"extract_job_info\", \"generate_cover_letters\")\n",
        "graph_builder.add_edge(\"generate_cover_letters\", \"report\")\n",
        "graph_builder.add_edge(\"report\", END)\n",
        "\n",
        "graph = graph_builder.compile(\n",
        "    checkpointer=memory,\n",
        ")\n"
      ],
      "metadata": {
        "id": "pXNaBV2RM0lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show mermaid diagram\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "TzvIfsssNOdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute Graph\n",
        "\n",
        "import asyncio\n",
        "from playwright.async_api import BrowserContext\n",
        "\n",
        "async def run_graph():\n",
        "  # Create Playwright globals\n",
        "  async with async_playwright() as playwright:\n",
        "    # Init browser, page\n",
        "    global browser, page\n",
        "    browser = await playwright.chromium.launch(headless=False)\n",
        "    context: BrowserContext = await browser.new_context(\n",
        "        viewport={\"width\": 3840, \"height\": 2160}  # Set screen resolution\n",
        "    )\n",
        "    page = await context.new_page()\n",
        "\n",
        "    # Init state and config\n",
        "    initial_state = {\"jobs\": []}\n",
        "    config = {\"configurable\": {\"thread_id\": \"1\"}, \"page\" : page}\n",
        "\n",
        "    # Start the graph\n",
        "    async for event in graph.astream(initial_state, config, stream_mode=\"updates\"):\n",
        "        print(event)\n",
        "\n",
        "    print(\"All done!\")\n",
        "\n",
        "# Kick everything off\n",
        "await run_graph()\n",
        "\n"
      ],
      "metadata": {
        "id": "o-yz2DhoNQs_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}